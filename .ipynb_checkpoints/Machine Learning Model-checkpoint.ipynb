{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a62ecd-81cf-4fce-b77c-e6f68b492a79",
   "metadata": {},
   "source": [
    "# Machine Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113c31e-429f-4f80-8e75-252aef6e3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local libraries\n",
    "import Tools.ratings_utils as ru\n",
    "import Tools.system_utils as sys\n",
    "import Tools.season_utils as su\n",
    "\n",
    "# Third party packages\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "YEARS = [2021]\n",
    "# YEARS = [2021, 2022, 2023, 2024, 2025]\n",
    "WRITE_TO_CSV = False\n",
    "\n",
    "\n",
    "_, tournament_filename, picks_filename, ratings_filename = su.create_filenames(years=YEARS)\n",
    "\n",
    "# Create data frame for valid teams in the current season that can be used for tournament simulation\n",
    "score_df = ru.create_score_df(years=YEARS)\n",
    "rating_score_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012d948-fd2e-4011-ba53-7128264b3fd3",
   "metadata": {},
   "source": [
    "# Logistic Regression Model - Option #1\n",
    "### Home team is winner -> 1\n",
    "### Away team is winner -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab30286e-ee4d-46f0-8835-94f3c4d4b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from JSON\n",
    "rating_score_df = pd.read_json(ratings_filename)\n",
    "\n",
    "# Set data frame and target variable\n",
    "df = rating_score_df.copy()\n",
    "df[\"y\"] = (df[\"Winner\"] == df[\"Home\"]).astype(int)\n",
    "\n",
    "# Add feature columns\n",
    "df = ru.derive_features(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d852d00-0ee0-4e02-98dc-0d5c7952a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features\n",
    "features = ru.ML_FEATURES\n",
    "\n",
    "# Create X, y data frames\n",
    "X = df[features]\n",
    "y = df[\"y\"]\n",
    "\n",
    "# Split train/test data sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b26fc-a881-448b-a37a-545743828fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a324e-0109-4e7a-bce3-6cdd66e36ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)\n",
    "y_prob = log_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9ffd2-e64c-42db-a34e-43a40fe577fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Print neatly\n",
    "print(f\"Logistic Regression Model\\nPerformance Metrics:\")\n",
    "print(f\"---------------------------\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(f\"ROC AUC  : {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396405f-5d69-4fb0-a671-42d4677d0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": log_model.coef_[0]\n",
    "})\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440007d-c44b-4497-9f2c-d799b569dd6c",
   "metadata": {},
   "source": [
    "# Test Logistic Regression Model Against March Madness Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ea948-0262-44e2-8c1c-4581946e2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dict = ru.compile_ratings_dict(score_df=score_df)\n",
    "\n",
    "_, _, tourney_dict, results = ru.simulate_tournament_with_all_ratings(\n",
    "    filename=tournament_filename,\n",
    "    ratings=ratings_dict,\n",
    "    model=log_model)\n",
    "\n",
    "print(results)\n",
    "\n",
    "if WRITE_TO_CSV:\n",
    "    sys.write_tournament_to_csv(tourney_dict=tourney_dict,\n",
    "                                filename=picks_filename,\n",
    "                                rating_type=\"log_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f3c6a-62a8-4465-b08c-c008dd269551",
   "metadata": {},
   "source": [
    "# XGBoost Model - Option #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d784c3e-88d3-424f-8c27-0c72de302f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from JSON\n",
    "rating_score_df = pd.read_json(ratings_filename)\n",
    "\n",
    "# Set data frame and target variable\n",
    "df = rating_score_df.copy()\n",
    "df[\"y\"] = (df[\"Winner\"] == df[\"Home\"]).astype(int)\n",
    "\n",
    "# Add feature columns\n",
    "df = ru.derive_features(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f7a36-fc44-4a54-b153-734174dafbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features\n",
    "features = ru.ML_FEATURES\n",
    "\n",
    "# Create X, y data frames\n",
    "X = df[features]\n",
    "y = df[\"y\"]\n",
    "\n",
    "# Split train/test data sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43e625-f560-4bda-bb38-b875a12b5162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\",\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d363c0d-e274-4db5-8a1b-8928c83b2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_prob = xgb_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fca5c2-ca03-49fc-ada8-7e16d36d56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Print neatly\n",
    "print(f\"XGBoost Model\\nPerformance Metrics:\")\n",
    "print(f\"---------------------------\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(f\"ROC AUC  : {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318a870-323b-4a8a-8bf6-716ae0761d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgb_model, importance_type=\"gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a473d8-342b-460f-9f7c-a9dd84d56f44",
   "metadata": {},
   "source": [
    "# Test XGBoost Model Against March Madness Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c121b-1e00-46f7-87fb-f83b7910d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dict = ru.compile_ratings_dict(score_df=score_df)\n",
    "\n",
    "_, _, tourney_dict, results = ru.simulate_tournament_with_all_ratings(\n",
    "    filename=tournament_filename,\n",
    "    ratings=ratings_dict,\n",
    "    model=xgb_model)\n",
    "\n",
    "print(results)\n",
    "\n",
    "if WRITE_TO_CSV:\n",
    "    sys.write_tournament_to_csv(tourney_dict=tourney_dict,\n",
    "                                filename=picks_filename,\n",
    "                                rating_type=\"xgb_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e9450-4774-4fb5-920f-89deb284c8e1",
   "metadata": {},
   "source": [
    "# Random Forest - Option #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a37980-f692-4c22-8418-654750a0a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from JSON\n",
    "rating_score_df = pd.read_json(ratings_filename)\n",
    "\n",
    "# Set data frame and target variable\n",
    "df = rating_score_df.copy()\n",
    "df[\"y\"] = (df[\"Winner\"] == df[\"Home\"]).astype(int)\n",
    "\n",
    "# Add feature columns\n",
    "df = ru.derive_features(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d619646-b5bc-4ef7-b712-bec06060b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features\n",
    "features = ru.ML_FEATURES\n",
    "\n",
    "# Create X, y data frames\n",
    "X = df[features]\n",
    "y = df[\"y\"]\n",
    "\n",
    "# Split train/test data sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b4a6a-903f-49db-9c1c-efd586cf8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,      # number of trees\n",
    "    max_depth=None,       # let trees go deep until pure\n",
    "    min_samples_split=2,  # default\n",
    "    min_samples_leaf=1,   # default\n",
    "    max_features=\"sqrt\",  # good for classification\n",
    "    random_state=42,\n",
    "    n_jobs=-1             # use all cores\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a4489-6a98-4e5d-a6bd-b0e132de7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_proba = rf_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715dad10-19dd-40ba-8b36-f41e534818ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Print neatly\n",
    "print(f\"Random Forest Model\\nPerformance Metrics:\")\n",
    "print(f\"---------------------------\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(f\"ROC AUC  : {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f64e5-0a23-4923-8fcb-a42ab5c5ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(rf_model.feature_importances_, index=X_train.columns)\n",
    "print(importances.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfc4e2-4f65-481c-b60f-f2edec4322e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dict = ru.compile_ratings_dict(score_df=score_df)\n",
    "\n",
    "_, _, tourney_dict, results = ru.simulate_tournament_with_all_ratings(\n",
    "    filename=tournament_filename,\n",
    "    ratings=ratings_dict,\n",
    "    model=rf_model)\n",
    "\n",
    "print(results)\n",
    "\n",
    "if WRITE_TO_CSV:\n",
    "    sys.write_tournament_to_csv(tourney_dict=tourney_dict,\n",
    "                                filename=picks_filename,\n",
    "                                rating_type=\"rf_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7d54b-e366-4522-bc5e-39e8b3b67eeb",
   "metadata": {},
   "source": [
    "## Exploratory Data Anaysis per feature (Plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da0757-0b78-4c18-a9ba-2da4a6cc8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cd013-8963-4898-b90f-54a2be271293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from JSON\n",
    "rating_score_df = pd.read_json(ratings_filename)\n",
    "\n",
    "# Set data frame and target variable\n",
    "df = rating_score_df.copy()\n",
    "df[\"y\"] = (df[\"Winner\"] == df[\"Home\"]).astype(int)\n",
    "\n",
    "# Add feature columns\n",
    "df = ru.derive_features(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafae14-a2fe-4ffd-8e8b-2fdbd0d320f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_vs_win(df, feature_col, target_col='y'):\n",
    "    y = df[target_col].values\n",
    "    x = df[feature_col].values\n",
    "\n",
    "    # jitter Y so points don't stack\n",
    "    y_jitter = y + np.random.normal(0, 0.03, size=len(y))\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(x, y_jitter, alpha=0.4)\n",
    "    plt.yticks([0, 1], ['Loss', 'Win'])\n",
    "    plt.xlabel(feature_col)\n",
    "    plt.ylabel('Outcome')\n",
    "    plt.title(f'Win vs {feature_col}')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_binned_win_rate(df, feature_col, target_col='y', bins=10):\n",
    "    x = df[feature_col]\n",
    "    y = df[target_col]\n",
    "\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp['bin'] = pd.qcut(x, bins, duplicates='drop')\n",
    "\n",
    "    win_rate = df_tmp.groupby('bin')[target_col].mean()\n",
    "    bin_mid = [interval.mid for interval in win_rate.index]\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(bin_mid, win_rate, marker='o')\n",
    "    plt.xlabel(feature_col)\n",
    "    plt.ylabel('Win Probability')\n",
    "    plt.title(f'Binned Win Rate vs {feature_col}')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_binned_win_rate_fixed(df, feature_col, target_col='y', bins=10):\n",
    "    x = df[feature_col]\n",
    "    y = df[target_col]\n",
    "\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp['bin'] = pd.cut(x, bins=bins)\n",
    "\n",
    "    win_rate = df_tmp.groupby('bin')[target_col].mean()\n",
    "    bin_mid = [interval.mid for interval in win_rate.index]\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(bin_mid, win_rate, marker='o')\n",
    "    plt.xlabel(feature_col)\n",
    "    plt.ylabel('Home Win Probability')\n",
    "    plt.title(f'Binned Win Rate vs {feature_col}')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c4a14-97eb-4aea-b982-b13a8f9850fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run plots for target outcome (home team wins) against all features\n",
    "for feature in ru.ML_FEATURES:\n",
    "    # plot_feature_vs_win(df, feature)\n",
    "    # plot_binned_win_rate(df, feature)\n",
    "    plot_binned_win_rate_fixed(df, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6118b-5fc5-43ab-8782-0a1c034d6438",
   "metadata": {},
   "source": [
    "## Save Ratings to JSON\n",
    "### (Skip if already run for this season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757e321-a48f-4760-a8c9-4bb1c56caa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_score_df = ru.add_ratings_per_game(score_df=score_df)\n",
    "rating_score_df.to_json(ratings_filename, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a673911c-9a19-4497-9c95-c8163a102fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988cb02-0f9d-4cae-8f8a-870612530822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "college-hoops",
   "language": "python",
   "name": "college-hoops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
